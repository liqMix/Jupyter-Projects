{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nonbatch of CS410 - HW5",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkL62yxv9z5b",
        "colab_type": "text"
      },
      "source": [
        "#Reinforcement Learning - Tic Tac Toe\n",
        "##Brady Young"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxhwnwAd8u6V",
        "colab_type": "text"
      },
      "source": [
        "##Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr7ZMnvKSy0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import math\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIbeyHBd8zIF",
        "colab_type": "text"
      },
      "source": [
        "##Tic Tac Toe Game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux7luGQzSaN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TicTacToe:\n",
        "  def __init__(self, width=3, state=None):\n",
        "    #print(\"Creating board of size: \", str(width**2))\n",
        "    self.width = width\n",
        "    self.winner = None\n",
        "    self.done = False\n",
        "    self.rewards = { ' ':    2.0,  ## Draw\n",
        "                     'O':   10.0,  ## Win\n",
        "                     'X':  -30.0,  ## Lose\n",
        "                     None:   -1.0}  ## Time Step\n",
        "    \n",
        "    if state is None:\n",
        "      self.state = [[' ' for x in range(self.width)] for x in range(self.width)]\n",
        "    else:\n",
        "      self.state = copy.deepcopy(state)\n",
        "    \n",
        "  def __repr__(self):\n",
        "    for x in self.state:\n",
        "      print(x)\n",
        "    return ''\n",
        "  \n",
        "  def play_match(self, agent, opponent):\n",
        "    self.state = [[' ' for x in range(self.width)] for x in range(self.width)]\n",
        "    \n",
        "    self.agent_mark = agent.mark\n",
        "    self.opp_mark = opponent.mark\n",
        "    \n",
        "    players = random.sample((agent, opponent), k=2)\n",
        "    \n",
        "    ## While the game is not over\n",
        "    while not self.done:\n",
        "      prev_state = self.state\n",
        "      self.state = self.place_mark(players[0].choose_action(), players[0].mark)\n",
        "      agent.learn(prev_state, self.state)\n",
        "      self.is_done()\n",
        "      players.reverse()\n",
        "    return self.winner\n",
        "  \n",
        "    \n",
        "  def encode_state(self, state):\n",
        "    encoding = 0\n",
        "    values = { 'O':     1,\n",
        "               'X':  2}\n",
        "    w = self.width\n",
        "      \n",
        "    for i in range(w):\n",
        "      for j in range(w):\n",
        "        mark = state[i][j]\n",
        "        if mark is not ' ':\n",
        "          encoding += values[mark] * (3 ** ((i * w) + j))\n",
        "          \n",
        "    return int(encoding)\n",
        "  \n",
        "  def get_empty(self):\n",
        "    empty = []\n",
        "    for i in range(self.width):\n",
        "      for j in range(self.width):\n",
        "        if self.state[i][j] is ' ':\n",
        "          empty.append((i, j))\n",
        "    return empty\n",
        "  \n",
        "  \n",
        "  def is_full(self):\n",
        "    for cell in self.state:\n",
        "      for value in cell:\n",
        "        if value is ' ':\n",
        "          return False    \n",
        "    return True\n",
        "  \n",
        "  \n",
        "  def place_mark(self, pos, mark):\n",
        "    state = copy.deepcopy(self.state)\n",
        "    state[pos[0]][pos[1]] = mark\n",
        "    return state\n",
        "  \n",
        "  def is_done(self):\n",
        "    if not self.done:\n",
        "      if self.winner is None:\n",
        "        winner = self.find_vertical()\n",
        "        if winner is None:\n",
        "          winner = self.find_horizontal()\n",
        "          if winner is None:\n",
        "            winner = self.find_diagonal()\n",
        "            if winner is None:\n",
        "              if self.is_full() is True:\n",
        "                self.winner = ' '\n",
        "                self.done = True\n",
        "                return True\n",
        "              else:\n",
        "                return False\n",
        "        self.winner = winner\n",
        "        self.done = True\n",
        "    return True\n",
        "  \n",
        "  def get_reward(self, state):\n",
        "    temp = copy.deepcopy(self.state)\n",
        "    temp_win = self.winner\n",
        "    temp_done = self.done\n",
        "    \n",
        "    self.state = state\n",
        "    if not self.done:\n",
        "      self.is_done()\n",
        "      self.done = temp_done\n",
        "      \n",
        "    reward = self.rewards[self.winner]\n",
        "    self.state = temp\n",
        "    self.winner = temp_win\n",
        "    return reward\n",
        "  \n",
        "  \n",
        "  def find_vertical(self):\n",
        "    state = self.state\n",
        "    for i in range(self.width):\n",
        "      if state[0][i] is not ' ':\n",
        "        mark = state[0][i]\n",
        "        \n",
        "        for j in range(self.width):\n",
        "          if state[j][i] is not mark:\n",
        "            break\n",
        "          if j == self.width-1:\n",
        "            return mark\n",
        "          \n",
        "    return None\n",
        "  \n",
        "  \n",
        "  def find_horizontal(self):\n",
        "    state = self.state \n",
        "    for i in range(self.width):\n",
        "      if state[i][0] is not ' ':\n",
        "        mark = state[i][0]\n",
        "        \n",
        "        for j in range(self.width):\n",
        "          if state[i][j] is not mark:\n",
        "            break\n",
        "          if j == self.width-1:\n",
        "            return mark\n",
        "          \n",
        "    return None\n",
        "  \n",
        "  \n",
        "  def find_diagonal(self):\n",
        "    state = self.state\n",
        "    if state[0][0] is not ' ':\n",
        "      mark = state[0][0]\n",
        "      for i in range(self.width):\n",
        "        if state[i][i] is not mark:\n",
        "          break\n",
        "        if i == (self.width-1):\n",
        "          return mark\n",
        "      \n",
        "    if state[0][self.width-1] is not ' ':\n",
        "      mark = state[0][self.width-1]\n",
        "      for i in range(self.width):\n",
        "        if state[i][self.width-1-i] is not mark:\n",
        "          break\n",
        "        if i == (self.width-1):\n",
        "          return mark\n",
        "    \n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxAL8_-584o_",
        "colab_type": "text"
      },
      "source": [
        "## Player / Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkPG3xvpdJVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Player:\n",
        "  def __init__(self, board, mark):\n",
        "    self.board = board\n",
        "    self.mark = mark\n",
        "    self.stats = { 'won' :  0,\n",
        "                   'lost':  0,\n",
        "                   'draw':  0,\n",
        "                   'total': 0}\n",
        "    \n",
        "  def __repr__(self):\n",
        "    out = \"---Agent---\" + \"\\nGames Won:\\t\" + str(self.stats['won']) + \\\n",
        "        \"\\nGames Lost:\\t\"+ str(self.stats['lost'])+ \\\n",
        "        \"\\nGames Draw:\\t\"+ str(self.stats['draw'])+ \\\n",
        "        \"\\nWinning Percentage: \" + str(self.stats['won'] / self.stats['total'])\n",
        "\n",
        "    return out + '\\n'\n",
        "  \n",
        "  def reset_stats(self):\n",
        "    for s in self.stats:\n",
        "      s = 0\n",
        "      \n",
        "  def choose_action(self):\n",
        "    empty = self.board.get_empty()\n",
        "    return random.choice(empty)\n",
        "\n",
        "class Human(Player):\n",
        "  def __init__(self, board, mark):\n",
        "    super(Human, self).__init__(board, mark)\n",
        "    \n",
        "  def choose_action(self):\n",
        "    print(self.board)\n",
        "    choice = None\n",
        "    \n",
        "    while choice is None:\n",
        "      choice = input(\"Choose a position as: x,y \").split(',')\n",
        "      choice = (int(choice[1]), int(choice[0]))\n",
        "      if choice not in self.board.get_empty():\n",
        "        print(\"Position \", str(choice) , \"is occupied.\")\n",
        "        choice = None\n",
        "        \n",
        "    return choice\n",
        "        \n",
        "class Agent(Player):\n",
        "  def __init__(self, board, mark, epsilon=0.9, gamma=0.75):\n",
        "    super(Agent, self).__init__(board, mark)\n",
        "\n",
        "    self.epsilon = epsilon      ## Chance to use random action\n",
        "    self.gamma = gamma          ## Reward discount\n",
        "    self.state_memory = {}\n",
        "    self.total_games = 0\n",
        "      \n",
        "  def save(self):\n",
        "    with open('agent.pickle', mode='rb') as file:\n",
        "      file = pickle(self)\n",
        "  \n",
        "\n",
        "      \n",
        "  def learn(self, state, state_d):\n",
        "    state_key = self.board.encode_state(state)\n",
        "    state_d_key = self.board.encode_state(state_d)\n",
        "\n",
        "    self.add_state(state)\n",
        "    count, reward = self.state_memory[state_key]\n",
        "    \n",
        "    count += 1;\n",
        "    alpha = 1 / count;\n",
        "    \n",
        "    self.add_state(state_d)\n",
        "    count_d, reward_d = self.state_memory[state_d_key]\n",
        "    \n",
        "    if self.board.done is True:\n",
        "      reward = reward + (alpha * (self.board.get_reward(state) - reward))\n",
        "      self.state_memory[state_key] = (count, reward)\n",
        "      return \n",
        "    \n",
        "    reward = reward + alpha * (-1 + (self.gamma * reward_d) - reward)\n",
        "    self.state_memory[state_key] = (count, reward)\n",
        "    return\n",
        "\n",
        "  \n",
        "  def add_state(self, state):\n",
        "    s = self.board.encode_state(state)\n",
        "    if s not in self.state_memory:\n",
        "      self.state_memory[s] = (0, self.board.get_reward(state))\n",
        "\n",
        "    \n",
        "  def choose_action(self):\n",
        "    rand = np.random.random()\n",
        "    possible_moves = self.board.get_empty()\n",
        "\n",
        "    if rand > self.epsilon:\n",
        "      return random.choice(possible_moves)\n",
        "    \n",
        "    possible_rewards = []\n",
        "    for p_m in possible_moves:\n",
        "      state_d = self.board.place_mark(p_m, self.mark)\n",
        "      encoded = self.board.encode_state(state_d)\n",
        "      self.add_state(state_d)\n",
        "      possible_rewards.append(self.state_memory[encoded][1])\n",
        "    return possible_moves[possible_rewards.index(max(possible_rewards))]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2h5giBkG1bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## The recursive training function\n",
        "## \n",
        "## Runs through two separate training regiments,\n",
        "## decreasing the chance for random action choice\n",
        "## on each successive game\n",
        "##\n",
        "## Last game of each regiment has no random choices\n",
        "##\n",
        "class Recurser:\n",
        "  def __init__(self, agent, opponent):\n",
        "    self.count = 0\n",
        "    self.x_winner = True\n",
        "    initial_epsilon = agent.epsilon\n",
        "    \n",
        "    \n",
        "    ## Train with agent playing first\n",
        "    while self.x_winner:\n",
        "      self.x_winner = False\n",
        "      game = TicTacToe()\n",
        "      self.recurse(game, agent, opponent)\n",
        "      if agent.epsilon < 1.0: agent.epsilon += 0.0005\n",
        "    print(\"Finished training with agent starting.\")\n",
        "    \n",
        "    ## Train with opponent playing first\n",
        "    agent.epsilon = initial_epsilon\n",
        "    self.x_winner = True\n",
        "    while self.x_winner:\n",
        "      self.x_winner = False\n",
        "      game = TicTacToe()\n",
        "      self.recurse(game, opponent, agent)\n",
        "      if agent.epsilon < 1.0: agent.epsilon += 0.0005\n",
        "    print(\"Finished training with opponent starting.\")\n",
        "\n",
        "    \n",
        "  ## Main training loop\n",
        "  def recurse(self, game, player_one, player_two):\n",
        "    if game.is_done():\n",
        "      self.count += 1\n",
        "      return game.winner\n",
        "\n",
        "    ## If agent is player, choose an action based on policy\n",
        "    if type(player_one) is Agent:\n",
        "      # Engage loop by setting winner to X\n",
        "      winner = 'X'\n",
        "      while winner is 'X':\n",
        "        game_d = TicTacToe(state=game.state)\n",
        "        player_one.board = game_d\n",
        "        move = player_one.choose_action()\n",
        "        state_d = game_d.place_mark(move, player_one.mark) \n",
        "        player_one.learn(game.state, state_d)\n",
        "        game_d.state = state_d\n",
        "        winner = self.recurse(game_d, player_two, player_one)\n",
        "      return\n",
        "    \n",
        "    ## If opponent is player, iterate through possible states\n",
        "    ## until agent win or draw is found\n",
        "    for move in game.get_empty():\n",
        "      game_d = TicTacToe(state=game.state)\n",
        "      player_two.board = game_d\n",
        "\n",
        "      game_d.state = game_d.place_mark(move, player_one.mark)\n",
        "      player_two.learn(game.state, game_d.state)\n",
        "      winner = self.recurse(game_d, player_two, player_one)\n",
        "      if winner is 'X':\n",
        "        self.x_winner = True\n",
        "      elif winner in ('O', ' '):\n",
        "        return\n",
        "\n",
        "      \n",
        "## Wrapper for the recursive call\n",
        "def exaustive_train_agent(agent):\n",
        "  print(\"Beginning training...\")\n",
        "  r = Recurser(agent, Player(game, 'X'))\n",
        "  print(\"Training complete.\")\n",
        "\n",
        "\n",
        "## Test the agent against an opponent, tracking games played\n",
        "def test_agent(agent, opponent, n_games):\n",
        "  test_total = 0\n",
        "  while test_total < n_games:\n",
        "    game = TicTacToe()\n",
        "    agent.board = game\n",
        "    opponent.board = game\n",
        "    winner = game.play_match(agent, opponent)\n",
        "    if winner is agent.mark:\n",
        "      agent.stats['won'] += 1\n",
        "    elif winner is opponent.mark:\n",
        "      agent.stats['lost']+= 1\n",
        "    else:\n",
        "      agent.stats['draw'] += 1\n",
        "\n",
        "    agent.stats['total'] += 1\n",
        "\n",
        "    test_total += 1\n",
        "    \n",
        "    \n",
        "  print(agent)\n",
        "  agent.reset_stats()\n",
        "  \n",
        "  if type(opponent) is Human:\n",
        "    print(game)\n",
        "    print(game.winner + \" is Winner!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKiaV00w9c9q",
        "colab_type": "text"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdYqPqvg9h_2",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lArtkwcNL6QO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "game = TicTacToe()\n",
        "epsilon = 0.5\n",
        "gamma = 0.9\n",
        "agent = Agent(game, 'O', epsilon, gamma)\n",
        "\n",
        "exaustive_train_agent(agent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGE7UH_F9kSp",
        "colab_type": "text"
      },
      "source": [
        "## AI vs Random-Choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6XO01nPqk64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opponent = Player(game, 'X')\n",
        "test_agent(agent, opponent, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXG_DXzf9sg0",
        "colab_type": "text"
      },
      "source": [
        "## AI vs Human"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktnr6I0ln-zv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opponent = Human(game, 'X')\n",
        "test_agent(agent, opponent, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}